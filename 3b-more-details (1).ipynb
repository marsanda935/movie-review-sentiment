{"cells":[{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"3fRouymTCosP"},"source":["## Trouble installing fastai library?"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"CFZnUciXCosU"},"source":["Here is a [guide to troubleshooting](https://docs.fast.ai/troubleshoot.html) problems with fastai installation.  By far, the most common problem is having fastai installed for a different environment/different Python installation than the one your Jupyter notebook is using (you can have Python installed in multiple places on your computer and not even realize it!). Or, you might have different versions of fastai installed in your different environments/different Python installations (and the one you are running in Jupyter notebook could be out of date, even if you installed version 1.0 somewhere else). For both of these problems, please [see this entry](https://docs.fast.ai/troubleshoot.html#modulenotfounderror-no-module-named-fastaivision)."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"MXiEQ-HYCosW"},"source":["## More detail about randomized SVD"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"CN0RCNm0CosY"},"source":["I didn't cover how randomized SVD worked, because we aren't going to learn about it in detail in this course.  The main things I want you to know about randomized SVD are:\n","- **it is fast**\n","- **it gives us a truncated SVD** (whereas with traditional SVD, we are usually throwing away small singular values and their corresponding columns)\n","\n","If you were curious to know more, two keys are:\n","- It is often useful to be able to reduce dimensionality of data in a way that preserves distances. The Johnsonâ€“Lindenstrauss lemma is a classic result of this type.  [Johnson-Lindenstrauss Lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma): a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved (proof uses random projections).\n","- We haven't found a better general SVD method, we'll just use the method we have on a smaller matrix.  \n","\n","Below is an over-simplified version of `randomized_svd` (you wouldn't want to use this in practice, but it covers the core ideas).  The main part to notice is that we multiply our original matrix by a smaller random matrix (`M @ rand_matrix`) to produce `smaller_matrix`, and then use our same `np.linalg.svd` as before:\n","\n","```\n","def randomized_svd(M, k=10):\n","    m, n = M.shape\n","    transpose = False\n","    if m < n:\n","        transpose = True\n","        M = M.T\n","        \n","    rand_matrix = np.random.normal(size=(M.shape[1], k))  # short side by k\n","    Q, _ = np.linalg.qr(M @ rand_matrix, mode='reduced')  # long side by k\n","    smaller_matrix = Q.T @ M                              # k by short side\n","    U_hat, s, V = np.linalg.svd(smaller_matrix, full_matrices=False)\n","    U = Q @ U_hat\n","    \n","    if transpose:\n","        return V.T, s.T, U.T\n","    else:\n","        return U, s, V\n","```\n","\n","This code snippet is from this [randomized-SVD jupyter notebook](https://github.com/fastai/randomized-SVD/blob/master/Randomized%20SVD.ipynb) which was the demo I used for my PyBay talk on [Using randomness to make code much faster](https://www.youtube.com/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7)."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"2Fxg9TOYCosc"},"source":["## Bayes Theorem"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"v5lZd_E9Cosd"},"source":["Ex: Physicist Leonard Mlodinow tested positive for HIV in 1989.  \n","\tHis doctor said there was a 99.9% chance he had HIV.  \n","   \n","A = positive test results\n","B = having HIV\n","\n","\n","True positives: \t$P(A|B) = 99.9\\%$\n","\n","Prevalence:\t$P(B)= 0.01\\%$\n","\n","False positives:\t$P(A|B^C) = 0.1\\%$\n","\n","Was his doctor correct?"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"OXkXyPMnCosf"},"source":["This example is from the book:\n","\n","<img src=\"images/drunkards-walk.jpg\" alt=\"drunkard's walk\" style=\"width: 30%\"/>"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"eRf0mo9fCosg"},"source":["Bayes Theorem (for conditional probabilities): $$ P(A | B) P(B) = P(B | A) P(A) $$"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"1gw0SUO3Cosi"},"source":["### Answer"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"aZZLrcMvCosk"},"outputs":[],"source":["# Exercise\n","\n","# <img src=\"images/mlodinow-false-pos.png\" alt=\"Mlodinow\" style=\"width: 80%\"/>"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"2_OjINvzCosm"},"source":["## Derivation of Naive Bayes"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"vP1CJo7TCosn"},"source":["We want to calculate the probability that the review \"I loved it\" is positive.  Using Bayes Theorem, we can rewrite this:"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"6Vfp5cYZCoso"},"source":["$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}, | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$\n","\n","The \"naive\" part of Naive Bayes is that we will assume that the probabilities of the different words are all independent.\n","\n","$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"dLiWYj67Coso"},"source":["We do the same calculation to see how likely it is the review is negative, and then choose whichever is larger."]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"wn9UkqY6Cosp"},"source":["$$ P(\\text{neg} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{neg}) \\cdot P(\\text{\"loved\"} | \\text{neg}) \\cdot P(\\text{\"it\"} | \\text{neg}) \\cdot P(\\text{neg})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"EBZm6dBUCosq"},"source":["We will add one to avoid dividing by zero (or something close to it).  Similarly, we take logarithms to avoid multiplying by a lot of tiny values.  For the reasons we want to avoid this, please see the next section on numerical stability:"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"N_niZGqZCosq"},"source":["More reading: [Using log-probabilities for Naive Bayes](http://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf)"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"n6K_uUSHCosr"},"source":["## Numerical Stability"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"PoylKGsGCosr"},"source":["#### Exercise"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"JMt3UcgkCosr"},"source":["Take a moment to look at the function $f$ below.  Before you try running it, write on paper what the output would be of $x_1 = f(\\frac{1}{10})$.  Now, (still on paper) plug that back into $f$ and calculate $x_2 = f(x_1)$.  Keep going for 10 iterations.\n","\n","This example is taken from page 107 of *Numerical Methods*, by Greenbaum and Chartier."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"RRCMgoCUCoss"},"outputs":[],"source":["def f(x):\n","    if x <= 1/2:\n","        return 2 * x\n","    if x > 1/2:\n","        return 2*x - 1"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"IB4ZVOjMCost"},"source":["Only after you've written down what you think the answer should be, run the code below:"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"v3YP55mMCost","executionInfo":{"status":"ok","timestamp":1748004650760,"user_tz":-420,"elapsed":29,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}},"outputId":"ad97c22c-c265-40b7-f4e7-86fa5d35efd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.1\n","0.2\n","0.4\n","0.8\n","0.6000000000000001\n","0.20000000000000018\n","0.40000000000000036\n","0.8000000000000007\n","0.6000000000000014\n","0.20000000000000284\n","0.4000000000000057\n","0.8000000000000114\n","0.6000000000000227\n","0.20000000000004547\n","0.40000000000009095\n","0.8000000000001819\n","0.6000000000003638\n","0.2000000000007276\n","0.4000000000014552\n","0.8000000000029104\n","0.6000000000058208\n","0.20000000001164153\n","0.40000000002328306\n","0.8000000000465661\n","0.6000000000931323\n","0.20000000018626451\n","0.40000000037252903\n","0.8000000007450581\n","0.6000000014901161\n","0.20000000298023224\n","0.4000000059604645\n","0.800000011920929\n","0.6000000238418579\n","0.20000004768371582\n","0.40000009536743164\n","0.8000001907348633\n","0.6000003814697266\n","0.20000076293945312\n","0.40000152587890625\n","0.8000030517578125\n","0.600006103515625\n","0.20001220703125\n","0.4000244140625\n","0.800048828125\n","0.60009765625\n","0.2001953125\n","0.400390625\n","0.80078125\n","0.6015625\n","0.203125\n","0.40625\n","0.8125\n","0.625\n","0.25\n","0.5\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n"]}],"source":["x = 1/10\n","for i in range(80):\n","    print(x)\n","    x = f(x)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"HqRScQ0HCosu"},"source":["What went wrong?"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"mfAX1mnrCosv"},"source":["### Problem: math is continuous & infinite, but computers are discrete & finite"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"F7iUvJE0Cosv"},"source":["Two Limitations of computer representations of numbers:\n","1. they can't be arbitrarily large or small\n","2. there must be gaps between them\n","\n","The reason we need to care about accuracy, is because computers can't store infinitely accurate numbers.  It's possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error)."]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"Ac7wns5pCosw"},"source":["How computers store numbers:\n","\n","<img src=\"images/fpa.png\" alt=\"floating point\" style=\"width: 60%\"/>\n","\n","The *mantissa* can also be referred to as the *significand*."]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"vit63RdyCosw"},"source":["IEEE Double precision arithmetic:\n","- Numbers can be as large as $1.79 \\times 10^{308}$ and as small as $2.23 \\times 10^{-308}$.\n","- The interval $[1,2]$ is represented by discrete subset:\n","$$1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2$$\n","\n","- The interval $[2,4]$ is represented:\n","$$2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4$$\n"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"CzIzeMWqCosx"},"source":["Floats and doubles are not equidistant:\n","\n","<img src=\"images/fltscale-wh.png\" alt=\"floating point\" style=\"width: 100%\"/>\n","Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"qyoLhfy-Cosx"},"source":["**Machine Epsilon**\n","\n","Half the distance between 1 and the next larger number. This can vary by computer.  IEEE standards for double precision specify $$ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}$$"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"nh9IkXh9Cosy"},"source":["**Two important properties of Floating Point Arithmetic**:\n","\n","- The difference between a real number $x$ and its closest floating point approximation $fl(x)$ is always smaller than $\\varepsilon_{machine}$ in relative terms.  For some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}$, $$fl(x)=x \\cdot (1 + \\varepsilon)$$\n","\n","- Where * is any operation ($+, -, \\times, \\div$), and $\\circledast$ is its floating point analogue,\n","    $$ x \\circledast y = (x * y)(1 + \\varepsilon)$$\n","for some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}$\n","That is, every operation of floating point arithmetic is exact up to a relative error of size at most $\\varepsilon_{machine}$"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"TBHyCEPzCosy"},"source":["## Speed of different types of memory"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"9ez0EBqiCosz"},"source":["This course is 90% NLP and 10% things I want to make sure you see before the end of your MSDS."]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"MnoYuLB4Cos1"},"source":["Here are some *numbers everyone should know* (from the legendary [Jeff Dean](http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/stanford-295-talk.pdf)):\n","- L1 cache reference 0.5 ns\n","- L2 cache reference 7 ns\n","- Main memory reference/RAM 100 ns\n","- Send 2K bytes over 1 Gbps network 20,000 ns\n","- Read 1 MB sequentially from memory 250,000 ns\n","- Round trip within same datacenter 500,000 ns\n","- Disk seek 10,000,000 ns\n","- Read 1 MB sequentially from network 10,000,000 ns\n","- Read 1 MB sequentially from disk 30,000,000 ns\n","- Send packet CA->Netherlands->CA 150,000,000 ns\n","\n","And here is an updated, interactive [version](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html), which includes a timeline of how these numbers have changed.\n","\n","**Key take-away**: Each successive memory type is (at least) an order of magnitude worse than the one before it.  Disk seeks are **very slow**."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"C3_4CP-nCos2"},"source":["## Revisiting Naive Bayes in an Excel Spreadsheet"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"1sDRUC7GCos2"},"source":["Let's calculate naive bayes in a spreadsheet to get a more visual picture of what is going on.  Here's how I processed the data for this:"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"r2OKyx_tCos3"},"source":["### Loading our data"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"1QrruwblCotX"},"outputs":[],"source":["def get_term_doc_matrix(label_list, vocab_len):\n","    j_indices = []\n","    indptr = []\n","    values = []\n","    indptr.append(0)\n","\n","    for i, doc in enumerate(label_list):\n","        feature_counter = Counter(doc.data)\n","        j_indices.extend(feature_counter.keys())\n","        values.extend(feature_counter.values())\n","        indptr.append(len(j_indices))\n","\n","#     return (values, j_indices, indptr)\n","\n","    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n","                                   shape=(len(indptr) - 1, vocab_len),\n","                                   dtype=int)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"lmHFWJZ4Cotb"},"source":["### Getting data for our spreadsheet"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"XfdiHCirCotc"},"source":["To keep our spreadsheet manageable, we will just get the 40 shortest reviews:"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"GYBkjosZCotg"},"source":["Get counts for all vocab used in our selection of the 40 shortest reviews:"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"scrolled":true,"id":"RalLvVdWCoth","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748005617394,"user_tz":-420,"elapsed":33,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}},"outputId":"48015e32-e3d6-4f73-b025-b7902c46bb15"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'movie': 25, 'great': 15, 'bad': 8, 'ok': 7}\n"]}],"source":["# Assume movie_reviews is loaded and contains training data\n","# If you don't have movie_reviews loaded, you'll need to load it first\n","# from fastai.text import *\n","# path = untar_data(URLs.IMDB_SAMPLE)\n","# movie_reviews = TextClasDataBunch.from_csv(path, 'texts.csv')\n","\n","# Initialize an empty dictionary to store vocabulary and counts\n","vocab_used = {}\n","\n","# Example: Iterate through the training data and populate vocab_used\n","# This is a placeholder and you'll need to adapt it to your specific data structure\n","# for review in movie_reviews.train_ds.items:\n","#     for word in review.data: # Assuming review.data contains a list of word indices\n","#         word_str = movie_reviews.vocab.itos[word] # Convert index to string\n","#         vocab_used[word_str] = vocab_used.get(word_str, 0) + 1\n","\n","# Replace the above placeholder loop with your actual code to populate vocab_used.\n","# For demonstration, let's create a dummy vocab_used:\n","vocab_used = {'the': 100, 'a': 80, 'and': 75, 'movie': 25, 'great': 15, 'bad': 8, 'ok': 7, 'terrible': 5}\n","\n","\n","selected_vocab = {k: v for k, v in vocab_used.items() if 6 <= v < 30}\n","print(selected_vocab)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"jEZCkETYCoti"},"source":["Let's choose the words that are used at least 6 times (so not too rare), but less than 30 (so not too common).  You could try experimenting with different cut-off points on your own:"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"lufYt4FPCoti"},"outputs":[],"source":["interesting_inds = [key for key, val in vocab_used.items() if val < 30 and val >6]"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"uGinSVyxCotj","outputId":"a38e5a6d-e865-4277-e7e9-af0858e15533","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748005655248,"user_tz":-420,"elapsed":99,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":31}],"source":["len(interesting_inds)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"_TSmWLGuCotm"},"source":["I copied the vocab and text of the movie reviews directly from here to paste into the spreadsheet:"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"Lux7oKcjCotm","outputId":"4ad2f21d-fca3-49e2-d748-87808a674d3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748005799259,"user_tz":-420,"elapsed":20,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['movie', 'great', 'bad', 'ok']\n"]}],"source":["# movie_reviews is not defined, and interesting_inds already contains the vocabulary strings.\n","# We can directly use interesting_inds.\n","print(interesting_inds)"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"0q0sqPBcCotn","outputId":"2c9b9457-0c73-4f52-8bf7-35c35ddad6c2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748005829337,"user_tz":-420,"elapsed":69,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['movie', 'great', 'bad', 'ok']\n","['This is a great movie.', 'This movie was bad.', 'It was just ok.']\n"]}],"source":["# movie_reviews is not defined, and interesting_inds already contains the vocabulary strings.\n","# We can directly use interesting_inds.\n","print(interesting_inds)\n","\n","# Assuming 'inds' is defined elsewhere and contains the indices of the reviews you want to work with.\n","# If 'inds' is not defined, you need to define it based on your selection criteria (e.g., shortest reviews).\n","# For example, if you wanted the first 40 reviews:\n","# inds = list(range(40))\n","\n","# Assuming movie_reviews is loaded and accessible with an attribute like train_ds.items\n","# and each item has a 'text' attribute.\n","# If you don't have movie_reviews loaded, you'll need to load it first\n","# from fastai.text import *\n","# path = untar_data(URLs.IMDB_SAMPLE)\n","# movie_reviews = TextClasDataBunch.from_csv(path, 'texts.csv')\n","\n","# Assuming 'inds' is a list or array of indices of the reviews you want to select\n","# And assuming movie_reviews.train_ds.items is a list of review objects\n","# and each review object has a 'text' attribute\n","# list_text = [movie_reviews.train_ds.items[i].text for i in inds]\n","\n","# Since movie_reviews and inds are not defined in the provided context,\n","# we cannot generate list_text based on those.\n","# If you manually want to create a placeholder list_text for demonstration:\n","list_text = [\"This is a great movie.\", \"This movie was bad.\", \"It was just ok.\"]\n","\n","# If list_text was intended to hold the actual text of the reviews selected by 'inds',\n","# you would need to uncomment and adapt the code above based on how movie_reviews and inds are structured.\n","\n","print(list_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"5-dl80KXCoto","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1748006619205,"user_tz":-420,"elapsed":71,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}},"outputId":"17535fa6-74ff-4723-88e3-2187f668bbbd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I understand. I cannot provide assistance with this specific issue. Please provide me with Python code containing syntax errors if you'd like me to help.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}],"source":["\"I understand. I cannot provide assistance with this specific issue. Please provide me with Python code containing syntax errors if you'd like me to help.\""]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"wkauD3dVCotp"},"source":["#### Export to CSVs"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"CjIXl0VdCotq"},"source":["Let's export the term-document matrix and the labels to CSVs.  "]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"f6JzUyTrCotr"},"outputs":[],"source":["from IPython.display import FileLink, FileLinks"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"w1A5fN2uCots","outputId":"5197afff-5dea-4b35-9731-02592302a072","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1748006675219,"user_tz":-420,"elapsed":122,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["/content/y.csv"],"text/html":["<a href='y.csv' target='_blank'>y.csv</a><br>"]},"metadata":{},"execution_count":51}],"source":["from IPython.display import FileLink, FileLinks\n","import numpy as np\n","import scipy.sparse\n","from collections import Counter\n","# Assuming movie_reviews is loaded and contains training data and vocab\n","# If you don't have movie_reviews loaded, you'll need to load it first\n","# from fastai.text import *\n","# path = untar_data(URLs.IMDB_SAMPLE)\n","# movie_reviews = TextClasDataBunch.from_csv(path, 'texts.csv')\n","\n","# Re-define the get_term_doc_matrix function if it's not in the current scope\n","def get_term_doc_matrix(label_list, vocab_len):\n","    j_indices = []\n","    indptr = []\n","    values = []\n","    indptr.append(0)\n","\n","    for i, doc in enumerate(label_list):\n","        # Assuming doc.data contains numerical indices of words\n","        feature_counter = Counter(doc.data)\n","        j_indices.extend(feature_counter.keys())\n","        values.extend(feature_counter.values())\n","        indptr.append(len(j_indices))\n","\n","    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n","                                   shape=(len(indptr) - 1, vocab_len),\n","                                   dtype=int)\n","\n","# Assuming movie_reviews is loaded and has train_ds and vocab attributes\n","# and inds is defined as the indices of the selected reviews\n","# If inds is not defined, define it here based on your criteria\n","# inds = list(range(40)) # Example: selecting the first 40 reviews\n","\n","# Assuming movie_reviews.train_ds.items contains the review data\n","# If not, you need to adapt this to your data structure\n","# selected_reviews = [movie_reviews.train_ds.items[i] for i in inds]\n","# y = np.array([r.label for r in selected_reviews]) # Assuming review object has a label attribute\n","\n","# Since movie_reviews, inds, and selected_reviews are not defined,\n","# we cannot directly call get_term_doc_matrix on them.\n","# If you manually want to create dummy data for demonstration:\n","class DummyReview:\n","    def __init__(self, data, label):\n","        self.data = data # List of word indices\n","        self.label = label\n","\n","# Dummy review data with word indices\n","selected_reviews = [\n","    DummyReview([1, 5, 2, 3], 1), # \"This is a great movie.\"\n","    DummyReview([3, 4, 6], 0),   # \"This movie was bad.\"\n","    DummyReview([7, 8, 9], 1)    # \"It was just ok.\"\n","]\n","y = np.array([r.label for r in selected_reviews])\n","\n","# Dummy vocab with index to string mapping\n","class DummyVocab:\n","    def __init__(self, itos):\n","        self.itos = itos\n","\n","# Map indices to strings based on your dummy vocab_used\n","dummy_itos = ['the', 'a', 'and', 'movie', 'great', 'bad', 'ok', 'terrible', 'it', 'was', 'just']\n","movie_reviews_vocab = DummyVocab(dummy_itos)\n","vocab_len = len(movie_reviews_vocab.itos)\n","\n","\n","# Generate the term-document matrix using the dummy data\n","x = get_term_doc_matrix(selected_reviews, vocab_len)\n","\n","# Convert interesting_inds (list of word strings) to numerical indices\n","# interesting_inds = ['movie', 'great', 'bad', 'ok'] # Example from the previous cell\n","# You need to ensure interesting_inds is populated from your actual vocabulary selection\n","\n","# Assuming interesting_inds is a list of strings that exist in movie_reviews_vocab.itos\n","# Map the interesting word strings back to their indices in the vocabulary\n","interesting_indices_numerical = [movie_reviews_vocab.itos.index(word) for word in interesting_inds if word in movie_reviews_vocab.itos]\n","\n","\n","# Now x is the sparse matrix and interesting_indices_numerical contains the column indices to select\n","np.savetxt(\"x.csv\", x.todense()[:, interesting_indices_numerical], delimiter=\",\", fmt='%.14f')\n","FileLink('x.csv')\n","\n","np.savetxt(\"y.csv\", y, delimiter=\",\", fmt=\"%i\")\n","FileLink('y.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"xPRIW4e3Cott","outputId":"42793e51-c11a-4f3e-e07a-54d281af3d30","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1748006681751,"user_tz":-420,"elapsed":113,"user":{"displayName":"Marsanda Sekeroni","userId":"15888496423185528240"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["/content/y.csv"],"text/html":["<a href='y.csv' target='_blank'>y.csv</a><br>"]},"metadata":{},"execution_count":52}],"source":["np.savetxt(\"y.csv\", y, delimiter=\",\", fmt=\"%i\")\n","FileLink('y.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}